apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: hf-gguf-convert
  labels:
    app.kubernetes.io/version: "0.1"
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/categories: CLI
    tekton.dev/tags: download
    tekton.dev/platforms: "linux/amd64,linux/s390x,linux/ppc64le"
spec:
  description: >-
    Converts a huggingface model to GGUF using llama.cpp

  params:
  - name: image
    description: The container image to be used
    type: string
    default: "ghcr.io/ggerganov/llama.cpp:full"

  - name: vocab-type
    description: Model Vocab Type for convertion fallback, hfft (default), bpe or spm
    type: string
    default: "hfft"

  - name: llama-path
    description: Path to llama.cpp git checkout
    type: string
    default: "/app"

  workspaces:
  - name: input
    description: The folder containing the huggingface model

  # Convert using convert-hf-to-gguf.py, with fallback to convert.py
  steps:
  - name: convert-gguf
    image: "$(params.image)"
    imagePullPolicy: Always
    env:
      - name: MODEL_PATH
        value: $(workspaces.input.path)
      - name: VOCAB_TYPE
        value: $(params.vocab-type)
      - name: HF_HOME
        value: /tmp
      - name: TRANSFORMERS_CACHE
        value: /tmp
    script: |-
      #!/bin/bash
    
      # Autodetect Vocab if known
      # if [ -f "${MODEL_PATH}/tokenizer_config.json" ] ; then
      #   grep -q LlamaTokenizer "${MODEL_PATH}/tokenizer_config.json" && VOCAB_TYPE=bpe
      # fi
      rm -f ${MODEL_PATH}/added_tokens.json || true
      python3 convert-hf-to-gguf.py ${MODEL_PATH} || \
        python3 convert.py --outtype f16 ${MODEL_PATH} --pad-vocab --vocab-type ${VOCAB_TYPE}
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
      runAsGroup: 65534
    workingDir: $(params.llama-path)
