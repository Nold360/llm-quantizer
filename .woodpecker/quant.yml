---
variables:
  - &llama_image 'ghcr.io/ggerganov/llama.cpp:full'
  - &hf_image 'python:3.10-slim'
  - pre_cmds: &pre_cmds
    - export MODEL_NAME=$(echo $MODEL_ID | awk -F/ '{print $$NF}')
    - export MODEL_PATH="$${MODEL_DIR}/$${MODEL_NAME}"
    - export OUTPUT_DIR="$${MODEL_DIR}/quants/$${MODEL_NAME}"
    # FIXME: We should have an image will all deps

  - env: &env
    ## HF Repo path:
    # - MODEL_ID=LargeWorldModel/LWM-Text-Chat-512K
    - QUANTS=Q2_K Q4_K_M Q5_K_M Q6_K Q8_0
    - ORG=nold

    - THREADS=16
    - VOCAB_TYPE=hfft
    - MODEL_SUFFIX=GGUF

    - MODEL_DIR=/data
    - CACHE_DIR=/data/.cache
      
    - HF_HUB_DISABLE_TELEMETRY=1
    - LLAMA_PATH=/app

when:
  event: manual

steps:
  get_model:
    image: *hf_image
    environment:
      - <<: *env
      - HF_HUB_ENABLE_HF_TRANSFER=1
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - apt-get update && apt-get install -y --no-install-recommends gettext-base
      - pip3 install -U "huggingface_hub[cli]" hf_transfer
      #- git config --global credential.helper store
      - mkdir -p $${OUTPUT_DIR} $${CACHE_DIR}

      # Download Model
      - huggingface-cli download --cache-dir $${CACHE_DIR} --local-dir "$${MODEL_PATH}" "$${MODEL_ID}"

      # Copy README & Append Footer
      - >
        if [ ! -f $${OUTPUT_DIR}/README.md ] ; then 
          cp $(find $${MODEL_PATH} -maxdepth 1 -type f -iname readme.md |\
            head -1) $${OUTPUT_DIR}/README.md
          cat FOOTER.md | envsubst >> $${OUTPUT_DIR}/README.md
        fi

  convert:
    image: *llama_image
    environment:
      - <<: *env
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - >
        if [ -f "$${MODEL_PATH}/ggml-model-f16.gguf" ] ; then
          echo " ----> Skipping Converting..."
        else
          python3 $${LLAMA_PATH}/convert-hf-to-gguf.py $${MODEL_PATH} || \
            python3 $${LLAMA_PATH}/convert.py --outtype f16 $${MODEL_PATH} --pad-vocab --vocab-type $${VOCAB_TYPE}
        fi
    depends_on: 
      - get_model

  quantize:
    image: *llama_image
    environment:
      - <<: *env
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - >
        for QUANT in $${QUANTS}; do
          if [ -f "$${OUTPUT_DIR}/$${MODEL_NAME}_$${QUANT}.gguf" ] ; then
            echo " ---> Skipping Quant ${QUANT}..."
          else
            $${LLAMA_PATH}/quantize "$${MODEL_PATH}/ggml-model-f16.gguf" "$${OUTPUT_DIR}/$${MODEL_NAME}_$${QUANT}.gguf" "$${QUANT}" "$$THREADS"
          fi
        done
    depends_on: 
      - convert

  release:
    image: *hf_image
    environment:
      - <<: *env
      - HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1
      - HF_HUB_ENABLE_HF_TRANSFER=0
    secrets:
      - hf_token
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - pip3 install -U "huggingface_hub[cli]" hf_transfer

      - echo " ---> Releasing Quantized Model as $${ORG}/$${MODEL_NAME}-$${MODEL_SUFFIX} ..."
      - export HF_REPO_NAME="$${ORG}/$${MODEL_NAME}-$${MODEL_SUFFIX}" 
      - python3 scripts/upload.py
    depends_on: 
      - quantize
