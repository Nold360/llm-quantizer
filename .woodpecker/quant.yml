---
variables:
  - &llama_image 'ghcr.io/ggerganov/llama.cpp:full'
  - pre_cmds: &pre_cmds
    - export MODEL_NAME=$(echo $MODEL_ID | awk -F/ '{print $$NF}')
    - export MODEL_PATH="$${MODEL_DIR}/$${MODEL_NAME}"
    - export OUTPUT_DIR="$${MODEL_DIR}/quants/$${MODEL_NAME}"
    # FIXME: We should have an image will all deps
    - pip3 install -U "huggingface_hub[cli]" hf_transfer
    - git config --global credential.helper store
    - apt-get update && apt-get install -y --no-install-recommends gettext-base

  - env: &env
    ## HF Repo path:
    - MODEL_ID=ise-uiuc/Magicoder-S-DS-6.7B
    - QUANTS=Q2_K Q5_K_M Q4_K_M Q8_0
    - ORG=nold

    - THREADS=16
    - VOCAB_TYPE=hfft
    - MODEL_SUFFIX=GGUF

    - MODEL_DIR=/data
    - CACHE_DIR=/data/.cache
      
    - HF_HUB_ENABLE_HF_TRANSFER=1
    - HF_HUB_DISABLE_TELEMETRY=1
    - LLAMA_PATH=/app

# when:
#   event: manual

steps:
  get_model:
    image: *llama_image
    environment:
      - <<: *env
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - mkdir -p $${OUTPUT_DIR} $${CACHE_DIR}
      - env
      - huggingface-cli download --cache-dir $${CACHE_DIR} --local-dir "$${MODEL_PATH}" "$${MODEL_ID}"

  convert:
    image: *llama_image
    environment:
      - <<: *env
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - >
        if [ -f "$${MODEL_PATH}/ggml-model-f16.gguf" ] ; then
          echo " ----> Skipping Converting..."
        else
          python3 $${LLAMA_PATH}/convert-hf-to-gguf.py $${MODEL_PATH} || \
            python3 $${LLAMA_PATH}/convert.py --outtype f16 $${MODEL_PATH} --pad-vocab --vocab-type $${VOCAB_TYPE}
        fi
    depends_on: 
      - get_model

  quantize:
    image: *llama_image
    environment:
      - <<: *env
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - >
        for QUANT in $${QUANTS}; do
          if [ -f "$${OUTPUT_DIR}/$${MODEL_NAME}_$${QUANT}.gguf" ] ; then
            echo " ---> Skipping Quant ${QUANT}..."
          else
            $${LLAMA_PATH}/quantize "$${MODEL_PATH}/ggml-model-f16.gguf" "$${OUTPUT_DIR}/$${MODEL_NAME}_$${QUANT}.gguf" "$${QUANT}" "$$THREADS"
          fi
        done
    depends_on: 
      - convert

  release:
    image: *llama_image
    environment:
      - <<: *env
    secrets:
      - hf_token
    volumes:
      - data:/data
    commands:
      - <<: *pre_cmds
      - echo " ---> Releasing Quantized Model as $${ORG}/$${MODEL_NAME}-$${MODEL_SUFFIX} ..."
      - huggingface-cli login --token $${HF_TOKEN} --add-to-git-credential
      - cp $(find $${MODEL_PATH} -maxdepth 1 -type f -iname readme.md | head -1) $${OUTPUT_DIR}/README.md
      - cat FOOTER.md | envsubst >> $${OUTPUT_DIR}/README.md
      - huggingface-cli upload --repo-type model "$${ORG}/$${MODEL_NAME}-$${MODEL_SUFFIX}" "$${OUTPUT_DIR}" .
    depends_on: 
      - quantize
